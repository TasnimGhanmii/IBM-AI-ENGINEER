import torch
import torch.nn as nn
import torch.optim as optim
import random
import time
from tqdm import tqdm

from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torchtext.datasets import Multi30k
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader
from nltk.translate.bleu_score import sentence_bleu

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass

import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')


# ========== Toy RNN prediction example ==========

W_xh = torch.tensor(-10.0)
W_hh = torch.tensor(10.0)
b_h = torch.tensor(0.0)
h_prev = torch.tensor(-1.0)

X = [1, 1, -1, -1, 1, 1]

# Initialize an empty list to store the predicted state values
H_hat = []
t = 1
for x in X:
    # Assign the current data point to x_t
    print("t=", t)
    x_t = torch.tensor(float(x))
    # Print the value of the previous state (h at time t-1)
    print("h_t-1", h_prev.item())

    # Compute the current state (h at time t) using the RNN formula with tanh activation
    h_t = torch.tanh(x_t * W_xh + h_prev * W_hh + b_h)

    # Update h_prev to the current state value for the next iteration
    h_prev = h_t

    # Print the current input value (x at time t)
    print("x_t", x_t.item())

    # Print the computed state value (h at time t)
    print("h_t", h_t.item())
    print("\n")

    # Append the current state value to the H_hat list after converting it to integer
    H_hat.append(int(h_t.item()))
    t += 1


# ========== Encoder, Decoder, Seq2Seq Definitions ==========

class Encoder(nn.Module):
    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):
        super().__init__()

        self.hid_dim = hid_dim
        self.n_layers = n_layers

        self.embedding = nn.Embedding(vocab_len, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout_prob)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, input_batch):
        #input_batch = [src len, batch size]
        embed = self.dropout(self.embedding(input_batch))
        embed = embed.to(device)
        #outputs = [src len, batch size, hid dim]
        #hidden = [n layers, batch size, hid dim]
        #cell = [n layers, batch size, hid dim]
        outputs, (hidden, cell) = self.lstm(embed)

        return hidden, cell


class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()

        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers

        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)
        self.fc_out = nn.Linear(hid_dim, output_dim)
        self.softmax = nn.LogSoftmax(dim=1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell):
        #input = [batch size]
        #hidden = [n layers, batch size, hid dim]
        #cell = [n layers, batch size, hid dim]

        input = input.unsqueeze(0)
        #input = [1, batch size]

        embedded = self.dropout(self.embedding(input))
        #embedded = [1, batch size, emb dim]

        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        #output = [1, batch size, hid dim]
        #hidden = [n layers, batch size, hid dim]
        #cell = [n layers, batch size, hid dim]

        prediction_logit = self.fc_out(output.squeeze(0))
        prediction = self.softmax(prediction_logit)
        #prediction = [batch size, output dim]

        return prediction, hidden, cell


class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device, trg_vocab):
        super().__init__()

        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        self.trg_vocab = trg_vocab

        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must be equal!"
        assert encoder.n_layers == decoder.n_layers, \
            "Encoder and decoder must have equal number of layers!"

    def forward(self, src, trg, teacher_forcing_ratio = 0.5):
        #src = [src len, batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing
        #e.g. if teacher_forcing_ratio is 0.75 you use ground‚Äêtruth inputs 75% of the time

        batch_size = trg.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim

        #tensor to store decoder outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)

        #last hidden state of the encoder is used as the initial hidden state of the decoder
        hidden, cell = self.encoder(src)
        hidden = hidden.to(self.device)
        cell = cell.to(self.device)

        #first input to the decoder is the <bos> tokens
        input = trg[0, :]

        for t in range(1, trg_len):
            #insert input token embedding, previous hidden and previous cell states
            #receive output tensor (predictions) and new hidden and cell states
            output, hidden, cell = self.decoder(input, hidden, cell)

            #place predictions in a tensor holding predictions for each token
            outputs[t] = output

            #decide if you are going to use teacher forcing or not
            teacher_force = random.random() < teacher_forcing_ratio

            #get the highest predicted token from your predictions
            top1 = output.argmax(1)

            #if teacher forcing, use actual next token as next input
            #if not, use predicted token
            input = trg[t] if teacher_force else top1

        return outputs


# ========== Preprocessing, Vocabulary, Tokenization ==========

SRC_LANGUAGE = 'de'
TGT_LANGUAGE = 'en'

token_transform = {}
token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')
token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')

UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3
special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']

def yield_tokens(data_iter, language):
    for src_sentence, trg_sentence in data_iter:
        if language == SRC_LANGUAGE:
            yield token_transform[language](src_sentence)
        else:
            yield token_transform[language](trg_sentence)

vocab_transform = {}
for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:
    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))
    vocab_transform[ln] = build_vocab_from_iterator(
        yield_tokens(train_iter, ln),
        min_freq=1,
        specials=special_symbols,
        special_first=True
    )
    vocab_transform[ln].set_default_index(UNK_IDX)


def text_transform(sentence, language):
    tokens = token_transform[language](sentence)
    return [BOS_IDX] + [vocab_transform[language].stoi[token] for token in tokens] + [EOS_IDX]


def collate_fn(batch):
    src_batch = []
    trg_batch = []
    for src_sample, trg_sample in batch:
        src_indices = text_transform(src_sample, SRC_LANGUAGE)
        trg_indices = text_transform(trg_sample, TGT_LANGUAGE)
        src_batch.append(torch.tensor(src_indices, dtype=torch.long))
        trg_batch.append(torch.tensor(trg_indices, dtype=torch.long))
    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)
    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)
    return src_batch, trg_batch


def get_translation_dataloaders(batch_size=32):
    train_dataset = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))
    valid_dataset = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    return train_loader, valid_loader


# ========== Device & Model Initialization ==========

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])
OUTPUT_DIM = len(vocab_transform[TGT_LANGUAGE])
ENC_EMB_DIM = 128
DEC_EMB_DIM = 128
HID_DIM = 256
N_LAYERS = 1
ENC_DROPOUT = 0.3
DEC_DROPOUT = 0.3

encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)
decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)

seq2seq_model = Seq2Seq(encoder, decoder, device, trg_vocab=vocab_transform[TGT_LANGUAGE]).to(device)


def init_weights(m):
    for name, param in m.named_parameters():
        nn.init.uniform_(param.data, -0.08, 0.08)

seq2seq_model.apply(init_weights)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(seq2seq_model):,} trainable parameters')


optimizer = optim.Adam(seq2seq_model.parameters())

PAD_IDX = vocab_transform[TGT_LANGUAGE].stoi['<pad>']

criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)


def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs


# ========== Training and Evaluation Functions ==========

def train(model, iterator, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0

    train_iterator = tqdm(iterator, desc="Training", leave=False)

    for i, (src, trg) in enumerate(train_iterator):

        src = src.to(device)
        trg = trg.to(device)
        optimizer.zero_grad()

        output = model(src, trg)
        #trg = [trg len, batch size]
        #output = [trg len, batch size, output dim]

        output_dim_local = output.shape[-1]

        output_flat = output[1:].view(-1, output_dim_local)
        trg_flat = trg[1:].contiguous().view(-1)

        loss = criterion(output_flat, trg_flat)
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

        optimizer.step()

        train_iterator.set_postfix(loss=loss.item())

        epoch_loss += loss.item()

    return epoch_loss / len(iterator)


def evaluate(model, iterator, criterion):
    model.eval()
    epoch_loss = 0

    valid_iterator = tqdm(iterator, desc="Evaluating", leave=False)

    with torch.no_grad():
        for i, (src, trg) in enumerate(iterator):
            src = src.to(device)
            trg = trg.to(device)

            output = model(src, trg, teacher_forcing_ratio=0.0)
            #trg = [trg len, batch size]
            #output = [trg len, batch size, output dim]

            output_dim_local = output.shape[-1]
            output_flat = output[1:].view(-1, output_dim_local)
            trg_flat = trg[1:].contiguous().view(-1)

            loss = criterion(output_flat, trg_flat)
            valid_iterator.set_postfix(loss=loss.item())
            epoch_loss += loss.item()

    return epoch_loss / len(iterator)


# ========== Example usage: Train, Validate, Generate, BLEU ==========

if __name__ == "__main__":

    N_EPOCHS = 3
    CLIP = 1

    train_dataloader, valid_dataloader = get_translation_dataloaders(batch_size=32)

    best_valid_loss = float('inf')

    for epoch in range(N_EPOCHS):

        start_time = time.time()

        train_loss = train(seq2seq_model, train_dataloader, optimizer, criterion, CLIP)
        valid_loss = evaluate(seq2seq_model, valid_dataloader, criterion)

        end_time = time.time()

        epoch_mins, epoch_secs = epoch_time(start_time, end_time)

        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(seq2seq_model.state_dict(), 'RNN-TR-model.pt')

        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
        print(f'\tTrain Loss: {train_loss:.3f}')
        print(f'\t Val. Loss: {valid_loss:.3f}')

    # load best model
    seq2seq_model.load_state_dict(torch.load('RNN-TR-model.pt', map_location=device))

    def generate_translation(model, src_sentence, src_vocab, trg_vocab, max_len=50):
        model.eval()  # Set the model to evaluation mode

        with torch.no_grad():
            src_indices = text_transform(src_sentence, SRC_LANGUAGE)
            src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(1).to(device)

            hidden, cell = model.encoder(src_tensor)

            trg_indexes = [BOS_IDX]  # Start with <bos> token

            for _ in range(max_len):
                last_token = torch.tensor([trg_indexes[-1]], dtype=torch.long).to(device)
                output, hidden, cell = model.decoder(last_token, hidden, cell)

                pred_token = output.argmax(1).item()
                trg_indexes.append(pred_token)

                if pred_token == EOS_IDX:
                    break

            trg_tokens = [trg_vocab.get_itos()[i] for i in trg_indexes]
            # Remove <bos> and <eos>
            if trg_tokens and trg_tokens[0] == '<bos>':
                trg_tokens = trg_tokens[1:]
            if trg_tokens and trg_tokens[-1] == '<eos>':
                trg_tokens = trg_tokens[:-1]

            translation = " ".join(trg_tokens)
            return translation

    # Actual translation example
    src_sentence = 'Ein asiatischer Mann kehrt den Gehweg.'

    generated_translation = generate_translation(seq2seq_model,
                                                 src_sentence=src_sentence,
                                                 src_vocab=vocab_transform[SRC_LANGUAGE],
                                                 trg_vocab=vocab_transform[TGT_LANGUAGE],
                                                 max_len=12)
    print("Generated translation:", generated_translation)

    def calculate_bleu_score(generated_translation, reference_translations):
        # Convert the generated translations and reference translations into the expected format for sentence_bleu
        references = [reference.split() for reference in reference_translations]
        hypothesis = generated_translation.split()

        bleu_score = sentence_bleu(references, hypothesis)
        return bleu_score

    reference_translations = [
        "Asian man sweeping the walkway .",
        "An asian man sweeping the walkway .",
        "An Asian man sweeps the sidewalk .",
        "An Asian man is sweeping the sidewalk .",
        "An asian man is sweeping the walkway .",
        "Asian man sweeping the sidewalk ."
    ]

    bleu_score = calculate_bleu_score(generated_translation, reference_translations)
    print("BLEU Score:", bleu_score)
